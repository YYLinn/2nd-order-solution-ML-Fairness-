{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Bias in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we check for some common fairness evaluation metrics in two new datasets that we think may be good options to stress test our selected algorithms on. We are aiming to stress test these algorithms to see how good they are at reducing bias.\n",
    "\n",
    "We use disparate impact as the key metric for how fair the data inherently. It is represented as $\\frac{P(Y = 1 | unpriviledged\\;group)}{P(Y = 1 | priviledged\\;group)}$, where $Y=1$ represents the probability of receiving the positive outcome. We use the AIF360 package to calculate this metric.\n",
    "\n",
    "- The first dataset is the adult dataset, which predicts whether someone's income exceeds $50k based on census data. The sensitive variables in this dataset are race, sex, marital status, age, native country. This dataset has a pretty comprehensive set of protected variables that we can test for bias with, but the problem with the data is that the target variable – assessing income – differs quite a bit from the objective of our clients. \n",
    "\n",
    "- The second dataset we are testing is the PDKK dataset, which is a credit lending dataset using Brazilian company data. The main protected variable here is sex, we have a marital status variable but there's no information on how it's encoded so it's pretty much useless to us. \n",
    "\n",
    "- The third datset we are testing is the 2018 Home Mortgage Disclosure Act data. It has a lot of information on the mortage themselves and credit characteristics of the borrowers as well as the borrowers' races, genders and age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Dataset repo from UCI\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from aif360.datasets import AdultDataset, StandardDataset\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "adult = AdultDataset()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a binary label fairness metric object using the AIF 360 package and fit it using the Adult dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact based on intersection of race and sex: 0.235.\n",
      "Disparate Impact based on race alone: 0.604.\n",
      "Disparate Impact based on sex alone: 0.363.\n"
     ]
    }
   ],
   "source": [
    "### Get disparate impact on the intersection of race and sex\n",
    "adult_fairness_metric = BinaryLabelDatasetMetric(\n",
    "    adult,\n",
    "    unprivileged_groups=[{\"race\":0, \"sex\":0}],\n",
    "    privileged_groups=[{\"race\":1, \"sex\":1}]\n",
    ")\n",
    "print(\n",
    "    \"Disparate Impact based on intersection of race and sex: \" +\n",
    "    f\"{adult_fairness_metric.disparate_impact():.3f}.\")\n",
    "\n",
    "### Get disparate impact based on race only\n",
    "adult_fairness_metric = BinaryLabelDatasetMetric(\n",
    "    adult,\n",
    "    unprivileged_groups=[{\"race\":0}],\n",
    "    privileged_groups=[{\"race\":1}]\n",
    ")\n",
    "print(\n",
    "    \"Disparate Impact based on race alone: \" +\n",
    "    f\"{adult_fairness_metric.disparate_impact():.3f}.\")\n",
    "\n",
    "### Get disparate impact based on sex only\n",
    "adult_fairness_metric = BinaryLabelDatasetMetric(\n",
    "    adult,\n",
    "    unprivileged_groups=[{\"sex\":0}],\n",
    "    privileged_groups=[{\"sex\":1}]\n",
    ")\n",
    "print(\n",
    "    \"Disparate Impact based on sex alone: \" +\n",
    "    f\"{adult_fairness_metric.disparate_impact():.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDKK Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/23x0hjl172sgh7hhvyngkf9m0000gn/T/ipykernel_65887/309645807.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pdkk = pd.read_table(\n"
     ]
    }
   ],
   "source": [
    "pdkk = pd.read_table(\n",
    "    \"../data/PAKDD2010_Modeling_Data.txt\", delimiter='\\t', encoding='latin1', \n",
    "    names=['ID_CLIENT', 'CLERK_TYPE', 'PAYMENT_DAY', 'APPLICATION_SUBMISSION_TYPE', \n",
    "    'QUANT_ADDITIONAL_CARDS', 'POSTAL_ADDRESS_TYPE', 'SEX', 'MARITAL_STATUS', \n",
    "    'QUANT_DEPENDANTS', 'EDUCATION_LEVEL', 'STATE_OF_BIRTH', 'CITY_OF_BIRTH', \n",
    "    'NACIONALITY', 'RESIDENCIAL_STATE', 'RESIDENCIAL_CITY', 'RESIDENCIAL_BOROUGH', \n",
    "    'FLAG_RESIDENCIAL_PHONE', 'RESIDENCIAL_PHONE_AREA_CODE', 'RESIDENCE_TYPE', \n",
    "    'MONTHS_IN_RESIDENCE', 'FLAG_MOBILE_PHONE', 'FLAG_EMAIL', \n",
    "    'PERSONAL_MONTHLY_INCOME', 'OTHER_INCOMES', 'FLAG_VISA', 'FLAG_MASTERCARD', \n",
    "    'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS', \n",
    "    'QUANT_BANKING_ACCOUNTS', 'QUANT_SPECIAL_BANKING_ACCOUNTS', \n",
    "    'PERSONAL_ASSETS_VALUE', 'QUANT_CARS', 'COMPANY', 'PROFESSIONAL_STATE', \n",
    "    'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'FLAG_PROFESSIONAL_PHONE', \n",
    "    'PROFESSIONAL_PHONE_AREA_CODE', 'MONTHS_IN_THE_JOB', 'PROFESSION_CODE', \n",
    "    'OCCUPATION_TYPE', 'MATE_PROFESSION_CODE', 'EDUCATION_LEVEL_MATE', \n",
    "    'FLAG_HOME_ADDRESS_DOCUMENT', 'FLAG_RG', 'FLAG_CPF', 'FLAG_INCOME_PROOF', \n",
    "    'PRODUCT', 'FLAG_ACSP_RECORD', 'AGE', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3', 'TARGET_LABEL'])\n",
    "\n",
    "# Clean the sex column by removing the Ns and empty entries\n",
    "pdkk = pdkk.loc[(pdkk[\"SEX\"] != \"N\") & (pdkk[\"SEX\"] != \" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate the PDKK dataset as a BinaryLabelDataset\n",
    "\n",
    "# Protected attributes\n",
    "protected_attributes = ['SEX']\n",
    "# We keep certain features \n",
    "selected_features = ['SEX', 'AGE', 'TARGET_LABEL']\n",
    "# Priviledged classes\n",
    "privileged_classes = [[\"M\"]]\n",
    "# Favorble Target label\n",
    "favorable_target_label = [0]\n",
    "# Create the binary label dataset object\n",
    "pdkk_dataset = StandardDataset(\n",
    "    df = pdkk,\n",
    "    label_name = \"TARGET_LABEL\",\n",
    "    favorable_classes = favorable_target_label,\n",
    "    protected_attribute_names = protected_attributes,\n",
    "    privileged_classes = privileged_classes,\n",
    "    features_to_keep = selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.028081281563987"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privileged_group = [{\"SEX\": 1}]\n",
    "unprivileged_group = [{\"SEX\": 0}]\n",
    "\n",
    "pdkk_metric = BinaryLabelDatasetMetric(\n",
    "    pdkk_dataset,\n",
    "    unprivileged_groups=unprivileged_group,\n",
    "    privileged_groups=privileged_group\n",
    ")\n",
    "\n",
    "pdkk_metric.disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much disparate impact there fortunately for the brazillian population and unfortunately for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMDA\n",
    "\n",
    "This dataset is from the 2018 Home Mortgage Disclosure Act (HMDA). \n",
    "\n",
    "Filtering Out Specific Loans:\n",
    "\n",
    "- Loans that were withdrawn by the applicants are removed. This is because withdrawn applications do not provide information about the lending decision.\n",
    "- Loans that were closed due to incompleteness are also removed, as these did not reach a decision point due to missing information.\n",
    "- Loans purchased by other institutions are excluded because the focus is on original lending decisions, not secondary market transactions.\n",
    "\n",
    "Mapping Action Outcomes:\n",
    "\n",
    "- The action_taken variable is recoded to simplify the analysis. Loans with outcomes of 1 (loan originated), 2 (application approved but not accepted), and 8 (preapproval request approved but not accepted) are mapped to 1, indicating approval.\n",
    "- Outcomes of 3 (application denied by financial institution) and 7 (preapproval request denied by financial institution) are mapped to 0, indicating denial.\n",
    "This step effectively transforms the action_taken variable into a binary indicator of loan approval status.\n",
    "\n",
    "Recode the Race Variable:\n",
    "\n",
    "- Observations where the race is not available or is listed as \"Joint\" are removed to focus on applications with clear racial identification. \n",
    "- The race variable is then recoded into a binary format, where \"White\" is coded as 0 and all other non-white races are combined and coded as 1. This simplifies the analysis to a comparison between White and Non-White applicants.\n",
    "\n",
    "Recode the Sex Variable:\n",
    "\n",
    "- Similar to race, observations where sex is not available or listed as \"Joint\" are removed.\n",
    "- The sex variable is recoded into a binary format, with \"Male\" coded as 0 and \"Female\" as 1, facilitating gender-based analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/23x0hjl172sgh7hhvyngkf9m0000gn/T/ipykernel_36689/1934115367.py:2: DtypeWarning: Columns (22,23,24,26,27,28,29,30,31,32,33,38,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hmda = pd.read_csv(\"../data/HMDA 2022 New Jersey.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resulting shape of the dataframe is (136442, 99)\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "hmda = pd.read_csv(\"../data/HMDA 2022 New Jersey.csv\")\n",
    "\n",
    "# Take out loans that were withdrawn by applicants\n",
    "hmda = hmda.loc[hmda[\"action_taken\"] != 4]\n",
    "# Take out loans that were closed for incompleteness\n",
    "hmda = hmda.loc[hmda[\"action_taken\"] != 5]\n",
    "# Take out loans that were purchased by other institutions\n",
    "hmda = hmda.loc[hmda[\"action_taken\"] != 6]\n",
    "\n",
    "# Map action outcomes 1, 2, 8 as approved\n",
    "hmda[\"action_taken\"] = hmda[\"action_taken\"].replace([1, 2, 8], 1)\n",
    "hmda[\"action_taken\"] = hmda[\"action_taken\"].replace([3, 7], 0)\n",
    "\n",
    "# Recode the race variable\n",
    "hmda = hmda.loc[hmda[\"derived_race\"] != \"Race Not Available\"]\n",
    "hmda = hmda.loc[hmda[\"derived_race\"] != \"Joint\"]\n",
    "\n",
    "# Replace all non-white observations as non-white\n",
    "hmda.loc[hmda[\"derived_race\"] != \"White\", \"derived_race\"] = \"Non-White\"\n",
    "hmda[\"derived_race\"] = hmda[\"derived_race\"].map({\"White\": 0, \"Non-White\": 1})\n",
    "\n",
    "# Recode the sex variable\n",
    "hmda = hmda.loc[hmda[\"derived_sex\"] != \"Sex Not Available\"]\n",
    "hmda = hmda.loc[hmda[\"derived_sex\"] != \"Joint\"]\n",
    "\n",
    "# Encode into 0 and 1\n",
    "hmda[\"derived_sex\"] = hmda[\"derived_sex\"].map({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "print(f\"The resulting shape of the dataframe is {hmda.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a standard dataset with the HDMA data\n",
    "\n",
    "# Protected attributes\n",
    "protected_attributes = ['derived_sex', 'derived_race']\n",
    "# We keep certain features \n",
    "selected_features = ['derived_sex', 'derived_race', 'action_taken']\n",
    "# Priviledged classes\n",
    "privileged_classes = [[0,0]]\n",
    "# Favorble Target label\n",
    "favorable_target_label = [1]\n",
    "# Create the binary label dataset object\n",
    "hmda_dataset = StandardDataset(\n",
    "    df = hmda,\n",
    "    label_name = \"action_taken\",\n",
    "    favorable_classes = favorable_target_label,\n",
    "    protected_attribute_names = protected_attributes,\n",
    "    privileged_classes = privileged_classes,\n",
    "    features_to_keep = selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact based on intersection of race and sex: 1.083.\n",
      "Disparate Impact based on sex alone: 0.993.\n",
      "Disparate Impact based on race alone: 1.086.\n"
     ]
    }
   ],
   "source": [
    "hmda_sex_and_race = BinaryLabelDatasetMetric(\n",
    "    hmda_dataset,\n",
    "    unprivileged_groups=[{\"derived_sex\": 0, \"derived_race\": 0}],\n",
    "    privileged_groups=[{\"derived_sex\": 1, \"derived_race\": 1}]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Disparate Impact based on intersection of race and sex: \" +\n",
    "    f\"{hmda_sex_and_race.disparate_impact():.3f}.\")\n",
    "\n",
    "hmda_sex = BinaryLabelDatasetMetric(\n",
    "    hmda_dataset,\n",
    "    unprivileged_groups=[{\"derived_sex\": 0}],\n",
    "    privileged_groups=[{\"derived_sex\": 1}]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Disparate Impact based on sex alone: \" +\n",
    "    f\"{hmda_sex.disparate_impact():.3f}.\")\n",
    "\n",
    "hmda_race = BinaryLabelDatasetMetric(\n",
    "    hmda_dataset,\n",
    "    unprivileged_groups=[{\"derived_race\": 0}],\n",
    "    privileged_groups=[{\"derived_race\": 1}]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Disparate Impact based on race alone: \" +\n",
    "    f\"{hmda_race.disparate_impact():.3f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

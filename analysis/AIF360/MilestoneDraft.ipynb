{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "## xgboost model\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Dataset\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.algorithms.preprocessing import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Import tensorflow session\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr, unprivileged_groups, privileged_groups):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = 1\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "        ## Balanced Accuracy (TPR + TNR)/2\n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        ## Average Odds Difference (average difference between TPR and TNR of groups)\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        ## Whether ratio of favorable outcome is consistent\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        ## Difference in probability of receiving favorable outcome\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        ## Whether individual have equal chances of receiving positive outcome when they should\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        ## Measure inequality of model errors across group\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, x_name, y_left, y_left_name, y_right, y_right_name, title = None):\n",
    "    fig, ax1 = plt.subplots(figsize=(10,7))\n",
    "    ax1.plot(x, y_left)\n",
    "    ax1.set_xlabel(x_name, fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel(y_left_name, color='b', fontsize=16, fontweight='bold')\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylim(0.5, 0.8)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, y_right, color='r')\n",
    "    ax2.set_ylabel(y_right_name, color='r', fontsize=16, fontweight='bold')\n",
    "    if 'DI' in y_right_name:\n",
    "        ax2.set_ylim(0., 0.7)\n",
    "    else:\n",
    "        ax2.set_ylim(-0.25, 0.1)\n",
    "\n",
    "    best_ind = np.argmax(y_left)\n",
    "    ax2.axvline(np.array(x)[best_ind], color='k', linestyle=':')\n",
    "    ax2.yaxis.set_tick_params(labelsize=14)\n",
    "    ax2.grid(True)\n",
    "    if title is not None:\n",
    "        ax1.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the Four Pre-Processing Methods in AIF360\n",
    "\n",
    "This report is for the Milestone paper, here we compare as many methods as we can from AIF360 to get a comprehensive evaluation of how well the model performs on the Taiwanese dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dataset\n",
    "data = pd.read_csv(\n",
    "    \"../../data/default_clean_v1.csv\",\n",
    "    index_col=0,\n",
    "    dtype={'default':\"category\",\n",
    "           \"SEX\":\"category\",\n",
    "           \"EDUCATION\":\"category\",\n",
    "           \"MARRIAGE\":\"category\"}\n",
    ")\n",
    "\n",
    "# Convert the default, sex, education, marriage variables to categorical\n",
    "data[\"default\"] = data[\"default\"].cat.codes\n",
    "data[\"SEX\"] = data[\"SEX\"].cat.codes\n",
    "\n",
    "# Drop the extra default column\n",
    "data.drop(columns = [\"default payment next month\"], inplace = True)\n",
    "\n",
    "categorical_cols = [\"EDUCATION\", \"MARRIAGE\"]\n",
    "data_dum = pd.get_dummies(data, columns=categorical_cols)\n",
    "\n",
    "\n",
    "# Initiate the binary label dataset - AIF360 convention\n",
    "binary_data = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df = data_dum,\n",
    "    label_names = [\"default\"],\n",
    "    protected_attribute_names = [\"SEX\"],\n",
    ")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "data_train, data_test = binary_data.split([0.7], shuffle=True)\n",
    "\n",
    "# Set unpriviledged groups and priviledged groups\n",
    "privileged_groups = [{'SEX': 0}]\n",
    "unprivileged_groups = [{'SEX': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make base model to assess Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/genesisqu/miniconda3/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Vanilla XGBoost', 'balanced accuracy': 0.7090393182610983, 'disparate impact': 0.92911088770505}\n"
     ]
    }
   ],
   "source": [
    "X_train = data_train.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_train = data_train.convert_to_dataframe()[0][\"default\"]\n",
    "X_test = data_test.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_test = data_test.convert_to_dataframe()[0][\"default\"]\n",
    "\n",
    "## Build model \n",
    "xgb_model = xgb.XGBClassifier(random_state = 2023)\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "## Conduct Random Search\n",
    "random_search = RandomizedSearchCV(xgb_model, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv = 5, \n",
    "                                   scoring=\"roc_auc\",\n",
    "                                   random_state=2023)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb_original = random_search.best_estimator_\n",
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "## Generate Metrics\n",
    "val_metrics = test(dataset=data_test,\n",
    "                   model=best_xgb_original,\n",
    "                   thresh_arr=thresh_arr,\n",
    "                   unprivileged_groups=unprivileged_groups,\n",
    "                   privileged_groups=privileged_groups)\n",
    "\n",
    "lr_orig_best_ind = np.argmax(val_metrics['bal_acc'])\n",
    "original_metrics = {\n",
    "    \"name\":\"Vanilla XGBoost\",\n",
    "    \"balanced accuracy\":val_metrics[\"bal_acc\"][lr_orig_best_ind],\n",
    "    \"disparate impact\":val_metrics[\"disp_imp\"][lr_orig_best_ind]\n",
    "}\n",
    "print(original_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Reweighting Scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/genesisqu/miniconda3/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'XGBoost after Reweighting', 'balanced accuracy': 0.70948416804575, 'disparate impact': 0.9385394511281405}\n"
     ]
    }
   ],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "RW.fit(data_train)\n",
    "data_train_reweighed = RW.transform(data_train)\n",
    "\n",
    "X_train = data_train_reweighed.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_train = data_train_reweighed.convert_to_dataframe()[0][\"default\"]\n",
    "X_test = data_train_reweighed.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_test = data_train_reweighed.convert_to_dataframe()[0][\"default\"]\n",
    "\n",
    "## Build model \n",
    "xgb_model = xgb.XGBClassifier()\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "## Conduct Random Search\n",
    "fit_param = {\"sample_weight\":data_train_reweighed.instance_weights}\n",
    "random_search = RandomizedSearchCV(xgb_model, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv = 5, \n",
    "                                   scoring=\"roc_auc\", \n",
    "                                   random_state=2023)\n",
    "random_search.fit(X_train, y_train, **fit_param)\n",
    "best_xgb_reweighed = random_search.best_estimator_\n",
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "## Generate Metrics\n",
    "val_metrics_reweigh = test(dataset=data_test,\n",
    "                   #### Change Model ####\n",
    "                   model=best_xgb_reweighed,\n",
    "                   thresh_arr=thresh_arr,\n",
    "                   unprivileged_groups=unprivileged_groups,\n",
    "                   privileged_groups=privileged_groups)\n",
    "lr_reweight_best_ind = np.argmax(val_metrics_reweigh['bal_acc'])\n",
    "#### Change Metric name ####\n",
    "reweight_metrics = {\n",
    "    #### Change Name ####\n",
    "    \"name\":\"XGBoost after Reweighting\",\n",
    "    \"balanced accuracy\":val_metrics_reweigh[\"bal_acc\"][lr_reweight_best_ind],\n",
    "    \"disparate impact\":val_metrics_reweigh[\"disp_imp\"][lr_reweight_best_ind]\n",
    "}\n",
    "print(reweight_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: LFR\n",
    "\n",
    "Use Learned Feature Representation to create new fairer feature independent of protected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'XGBoost with LFR', 'balanced accuracy': 0.4965956512189765, 'disparate impact': 1.0497296357381716}\n"
     ]
    }
   ],
   "source": [
    "lfr = LFR(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "lfr.fit(data_train)\n",
    "data_train_lfr = lfr.transform(data_train)\n",
    "\n",
    "X_train = data_train_lfr.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_train = data_train_lfr.convert_to_dataframe()[0][\"default\"]\n",
    "X_test = data_train_lfr.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_test = data_train_lfr.convert_to_dataframe()[0][\"default\"]\n",
    "\n",
    "## Build model \n",
    "xgb_model = xgb.XGBClassifier(random_state = 2023)\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "## Conduct Random Search\n",
    "random_search = RandomizedSearchCV(xgb_model, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv = 5, \n",
    "                                   scoring=\"roc_auc\")\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb_lfr = random_search.best_estimator_\n",
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "## Generate Metrics\n",
    "val_metrics_lfr = test(dataset=data_test,\n",
    "                   #### Change Model ####\n",
    "                   model=best_xgb_lfr,\n",
    "                   thresh_arr=thresh_arr,\n",
    "                   unprivileged_groups=unprivileged_groups,\n",
    "                   privileged_groups=privileged_groups)\n",
    "lr_lfr_best_ind = np.argmax(val_metrics_lfr['bal_acc'])\n",
    "#### Change Metric name ####\n",
    "lfr_metrics = {\n",
    "    #### Change Name ####\n",
    "    \"name\":\"XGBoost with LFR\",\n",
    "    \"balanced accuracy\":val_metrics_lfr[\"bal_acc\"][lr_lfr_best_ind],\n",
    "    \"disparate impact\":val_metrics_lfr[\"disp_imp\"][lr_lfr_best_ind]\n",
    "}\n",
    "print(lfr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/genesisqu/miniconda3/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'XGBoost with Disparate Impact Remover', 'balanced accuracy': 0.7112519687906311, 'disparate impact': 0.9481187766809394}\n"
     ]
    }
   ],
   "source": [
    "di = DisparateImpactRemover(repair_level=1, sensitive_attribute=\"SEX\")\n",
    "data_train_di = di.fit_transform(data_train)\n",
    "\n",
    "X_train = data_train_di.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_train = data_train_di.convert_to_dataframe()[0][\"default\"]\n",
    "X_test = data_train_di.convert_to_dataframe()[0].drop(columns = [\"default\"])\n",
    "y_test = data_train_di.convert_to_dataframe()[0][\"default\"]\n",
    "\n",
    "## Build model \n",
    "xgb_model = xgb.XGBClassifier(random_state = 2023)\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "## Conduct Random Search\n",
    "random_search = RandomizedSearchCV(xgb_model, \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv = 5, \n",
    "                                   scoring=\"roc_auc\")\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb_di = random_search.best_estimator_\n",
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "## Generate Metrics\n",
    "val_metrics = test(dataset=data_test,\n",
    "                   #### Change Model ####\n",
    "                   model=best_xgb_di,\n",
    "                   thresh_arr=thresh_arr,\n",
    "                   unprivileged_groups=unprivileged_groups,\n",
    "                   privileged_groups=privileged_groups)\n",
    "lr_di_best_ind = np.argmax(val_metrics['bal_acc'])\n",
    "#### Change Metric name ####\n",
    "di_metrics = {\n",
    "    #### Change Name ####\n",
    "    \"name\":\"XGBoost with Disparate Impact Remover\",\n",
    "    ### Change the indices here ####\n",
    "    \"balanced accuracy\":val_metrics[\"bal_acc\"][lr_di_best_ind],\n",
    "    \"disparate impact\":val_metrics[\"disp_imp\"][lr_di_best_ind]\n",
    "}\n",
    "#### Change the print statement ####\n",
    "print(di_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Post-Processing Methods 1: Calibrated Equality of Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Calibrated Equality of Odds', 'balanced accuracy': 0.6183875942406194, 'disparate impact': 0.9169900305285925}\n"
     ]
    }
   ],
   "source": [
    "### Learn parameters to equalize odds and apply to create a new dataset\n",
    "dataset_test_cpp = data_test.copy(deepcopy=True)\n",
    "cost_constraint = \"fnr\"\n",
    "# Make predictions using original xgboost model\n",
    "y_pred_original_prob = best_xgb_original.predict_proba(data_test.features)[:,0]\n",
    "dataset_test_cpp.scores = y_pred_original_prob.reshape(-1,1)\n",
    "### Get labels\n",
    "best_threshold = thresh_arr[lr_orig_best_ind]\n",
    "y_test_pred_original = np.zeros_like(dataset_test_cpp.labels)\n",
    "y_test_pred_original[y_pred_original_prob >= best_threshold] = dataset_test_cpp.favorable_label\n",
    "y_test_pred_original[~(y_pred_original_prob >= best_threshold)] = dataset_test_cpp.unfavorable_label\n",
    "dataset_test_cpp.labels = y_test_pred_original\n",
    "\n",
    "\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=2023)\n",
    "\n",
    "# Fit and transform using calibrated equality of odds\n",
    "cpp = cpp.fit(data_test, dataset_test_cpp)\n",
    "data_test_cpp_new = cpp.predict(dataset_test_cpp)\n",
    "## Get classification accuracy and fairness metric\n",
    "cm_transf_valid = ClassificationMetric(data_test, data_test_cpp_new,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "cp_tp = cm_transf_valid.true_positive_rate()\n",
    "cp_tn = cm_transf_valid.true_negative_rate()\n",
    "cp_fp = cm_transf_valid.false_positive_rate()\n",
    "cp_fn = cm_transf_valid.false_negative_rate()\n",
    "cp_balanced_acc = (cp_tp + cp_tn)/2\n",
    "cp_metrics = {\n",
    "    #### Change Name ####\n",
    "    \"name\":\"Calibrated Equality of Odds\",\n",
    "    ### Change the indices here ####\n",
    "    \"balanced accuracy\":cp_balanced_acc,\n",
    "    \"disparate impact\":cm_transf_valid.disparate_impact()\n",
    "}\n",
    "print(cp_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reject Options Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Reject Option Classification', 'balanced accuracy': 0.708576116587641, 'disparate impact': 0.9405863176931226}\n"
     ]
    }
   ],
   "source": [
    "# roc = RejectOptionClassification(\n",
    "#     unprivileged_groups,\n",
    "#     privileged_groups\n",
    "# )\n",
    "# data_test_roc = roc.fit_predict(data_test, dataset_test_cpp)\n",
    "\n",
    "# ## Get classification accuracy and fairness metric\n",
    "# cm_roc = ClassificationMetric(data_test, \n",
    "#                               data_test_roc,\n",
    "#                              unprivileged_groups=unprivileged_groups,\n",
    "#                              privileged_groups=privileged_groups)\n",
    "roc_tp = cm_roc.true_positive_rate()\n",
    "roc_tn = cm_roc.true_negative_rate()\n",
    "\n",
    "roc_balanced_acc = (roc_tp + roc_tn)/2\n",
    "roc_metrics = {\n",
    "    #### Change Name ####\n",
    "    \"name\":\"Reject Option Classification\",\n",
    "    ### Change the indices here ####\n",
    "    \"balanced accuracy\":roc_balanced_acc,\n",
    "    \"disparate impact\":cm_roc.disparate_impact()\n",
    "}\n",
    "print(roc_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting parameters\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['savefig.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [original_metrics, reweight_metrics, lfr_metrics, di_metrics, cp_metrics, roc_metrics]\n",
    "super_dict = {}\n",
    "for d in dicts:\n",
    "    for k, v in d.items():  # d.items() in Python 3+\n",
    "        super_dict.setdefault(k, []).append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# # Serializing json\n",
    "# json_object = json.dumps(super_dict, indent=4)\n",
    " \n",
    "# # Writing to sample.json\n",
    "# with open(\"aif_compare.json\", \"w\") as outfile:\n",
    "#     outfile.write(json_object)\n",
    "\n",
    "# Opening JSON file\n",
    "with open('aif_compare.json', 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    aif_compare = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/23x0hjl172sgh7hhvyngkf9m0000gn/T/ipykernel_30999/3597319341.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize = (10,5), sharey = True)\n",
    "axes[0].barh(np.arange(len(aif_compare[\"name\"])), np.array(aif_compare[\"balanced accuracy\"]) * 100)\n",
    "axes[0].set_yticks(np.arange(len(aif_compare[\"name\"])), labels=aif_compare[\"name\"])\n",
    "axes[0].axvline(x = aif_compare[\"balanced accuracy\"][0] * 100, linestyle = \"dashed\", c = \"red\")\n",
    "axes[0].xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "axes[0].set_title(\"Balanced Accuracy\")\n",
    "axes[1].barh(np.arange(len(aif_compare[\"name\"])), np.abs(1 - np.array(aif_compare[\"disparate impact\"])) * 100)\n",
    "axes[1].set_yticks(np.arange(len(aif_compare[\"name\"])), labels=aif_compare[\"name\"])\n",
    "axes[1].axvline(x = (1 - aif_compare[\"disparate impact\"][0]) * 100, linestyle = \"dashed\", c = \"red\", label = \"Vanilla Baseline\")\n",
    "axes[1].set_title(\"|1 - Disparate Impact|\")\n",
    "axes[1].xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "axes[1].legend()\n",
    "fig.suptitle(\"AIF360 Fairness Mitigation Metrics\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
